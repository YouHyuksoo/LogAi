"""
@file backend/app/services/ingest_consumer.py
@description
Kafka Consumer for log ingestion pipeline.
Uses confluent-kafka for better Python 3.12 compatibility.

Receives logs from Redpanda (logs-raw topic), parses with Drain3,
and stores in ClickHouse.

Usage:
  python -m app.services.ingest_consumer
"""

import json
import logging
from datetime import datetime
from confluent_kafka import Consumer, KafkaError
from app.core.config import settings
from app.services.drain_parser import parser
from app.services.clickhouse_client import ch_client
from app.services.anomaly_detector import detector

# Logging Setup (ì‹œê°„ í¬í•¨ í¬ë§· - ê°•ì œ ì ìš©)
logger = logging.getLogger("ingest_consumer")
logger.setLevel(logging.INFO)

# ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ìƒˆë¡œ ì„¤ì •
if logger.handlers:
    logger.handlers.clear()

# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter(
    '%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
console_handler.setFormatter(console_formatter)
logger.addHandler(console_handler)

# ìƒìœ„ ë¡œê±°ë¡œ ì „íŒŒ ë°©ì§€ (ì¤‘ë³µ ë¡œê·¸ ë°©ì§€)
logger.propagate = False

def process_message(msg_value: str):
    """
    Parse log message from Vector and extract fields.

    Args:
        msg_value: Raw message string from Kafka

    Returns:
        Tuple of (timestamp, level, service, template_id, template, raw_message, params)
        or None if parsing fails
    """
    try:
        # UTF-8 BOM ì œê±° ë° ê³µë°± ì •ë¦¬
        clean_value = msg_value.strip()
        if clean_value.startswith('\ufeff'):
            clean_value = clean_value[1:]

        # JSON íŒŒì‹± (Vectorì—ì„œ ì „ì†¡ëœ ë¡œê·¸)
        log_data = json.loads(clean_value)

        # Extract fields
        raw_message = log_data.get("message", "")
        timestamp_str = log_data.get("timestamp")

        # í˜„ì¬ ì‹œê°„ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš© (Vector timestamp íŒŒì‹± ë¬¸ì œ í•´ê²°)
        timestamp = datetime.now()

        if timestamp_str:
            try:
                # Vectorì˜ to_string(now()) í˜•ì‹: "2026-01-03T20:06:21.066097100Z"
                # Python fromisoformatì€ ë‚˜ë…¸ì´ˆ(9ìë¦¬)ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë§ˆì´í¬ë¡œì´ˆ(6ìë¦¬)ë¡œ ìë¦„
                ts = timestamp_str.replace('Z', '+00:00').replace(' ', 'T')

                # ë‚˜ë…¸ì´ˆë¥¼ ë§ˆì´í¬ë¡œì´ˆë¡œ ë³€í™˜ (ì†Œìˆ˜ì  ì´í•˜ 9ìë¦¬ -> 6ìë¦¬)
                if '.' in ts:
                    parts = ts.split('.')
                    # +00:00 ë¶€ë¶„ ë¶„ë¦¬
                    if '+' in parts[1]:
                        frac, tz = parts[1].split('+', 1)
                        frac = frac[:6].ljust(6, '0')  # 6ìë¦¬ë¡œ ìë¥´ê³  ë¶€ì¡±í•˜ë©´ 0 ì±„ì›€
                        ts = f"{parts[0]}.{frac}+{tz}"
                    else:
                        frac = parts[1][:6].ljust(6, '0')
                        ts = f"{parts[0]}.{frac}"

                timestamp = datetime.fromisoformat(ts)
                # UTCë¥¼ ë¡œì»¬ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (KST = UTC+9)
                timestamp = timestamp.replace(tzinfo=None)  # timezone ì œê±°í•˜ì—¬ naive datetimeìœ¼ë¡œ
            except Exception as e:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                logger.debug(f"Timestamp parse failed: {timestamp_str}, error: {e}, using current time")

        service = log_data.get("service", "unknown")
        level = log_data.get("level", "INFO")

        # Drain3 parsing
        parsed = parser.parse(raw_message)

        # Convert ExtractedParameter objects to strings
        params = parsed["params"]
        if params:
            params_str = [str(p.value) if hasattr(p, 'value') else str(p) for p in params]
        else:
            params_str = []

        return (
            timestamp,
            level,
            service,
            parsed["template_id"],
            parsed["template"],
            raw_message,
            params_str
        )

    except Exception as e:
        logger.error(f"Error processing message: {e}")
        return None

def main():
    """
    Main consumer loop using confluent-kafka.
    Polls messages from Redpanda and inserts into ClickHouse in batches.
    """
    # Confluent Kafka Consumer configuration
    conf = {
        'bootstrap.servers': settings.REDPANDA_BROKER,
        'group.id': 'log_processor_group',
        'auto.offset.reset': 'latest',
        'enable.auto.commit': True,
    }

    consumer = Consumer(conf)
    consumer.subscribe(['logs-raw'])

    logger.info("Starting Log Ingestion Consumer (confluent-kafka)...")
    logger.info(f"Broker: {settings.REDPANDA_BROKER}")
    logger.info(f"Topic: logs-raw")

    batch = []
    BATCH_SIZE = 100
    LAST_FLUSH = datetime.now()

    try:
        while True:
            # Poll for messages (timeout 1 second)
            msg = consumer.poll(timeout=1.0)

            if msg is None:
                # No message, check if we need to flush batch
                if batch and (datetime.now() - LAST_FLUSH).total_seconds() > 1.0:
                    ch_client.insert_logs(batch)
                    # ë§ˆì§€ë§‰ ë¡œê·¸ì˜ ì„œë¹„ìŠ¤ëª… í‘œì‹œ
                    last_service = batch[-1][2] if batch else "unknown"
                    logger.info(f"ğŸ“¥ Inserted {len(batch)} logs | last: {last_service}")
                    batch = []
                    LAST_FLUSH = datetime.now()

                    # ì´ìƒ íƒì§€ ì‹¤í–‰ (timeout flush í›„ì—ë„)
                    try:
                        detector.detect()
                    except Exception as e:
                        logger.error(f"Anomaly detection failed: {e}")
                continue

            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    # End of partition, not an error
                    continue
                else:
                    logger.error(f"Kafka error: {msg.error()}")
                    continue

            # Process message
            try:
                msg_value = msg.value().decode('utf-8')
                processed = process_message(msg_value)
                if processed:
                    batch.append(processed)
            except Exception as e:
                logger.error(f"Error decoding message: {e}")
                continue

            # Bulk Insert Condition
            if len(batch) >= BATCH_SIZE or (datetime.now() - LAST_FLUSH).total_seconds() > 1.0:
                if batch:
                    ch_client.insert_logs(batch)
                    # ë§ˆì§€ë§‰ ë¡œê·¸ì˜ ì„œë¹„ìŠ¤ëª… í‘œì‹œ
                    last_service = batch[-1][2] if batch else "unknown"
                    logger.info(f"ğŸ“¥ Inserted {len(batch)} logs | last: {last_service}")
                    batch = []

                    # ì´ìƒ íƒì§€ ì‹¤í–‰ (ë°°ì¹˜ ì‚½ì… í›„)
                    try:
                        detector.detect()
                    except Exception as e:
                        logger.error(f"Anomaly detection failed: {e}")

                LAST_FLUSH = datetime.now()

    except KeyboardInterrupt:
        logger.info("Consumer stopped by user.")
    finally:
        consumer.close()
        logger.info("Consumer closed.")

if __name__ == "__main__":
    main()
