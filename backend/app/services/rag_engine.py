"""
@file backend/app/services/rag_engine.py
@description
RAG (Retrieval-Augmented Generation) ì—”ì§„ ëª¨ë“ˆì…ë‹ˆë‹¤.
Qdrant ë²¡í„° DBì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , ClickHouseì—ì„œ ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. **search_similar_incidents**: ìœ ì‚¬ ì¥ì•  ì‚¬ë¡€/ë§¤ë‰´ì–¼ ê²€ìƒ‰
2. **get_log_context**: íƒ€ì„ìŠ¤íƒ¬í”„ ì£¼ë³€ ë¡œê·¸ ì¡°íšŒ
3. **save_incident**: ì¥ì•  ì‚¬ë¡€ë¥¼ Qdrantì— ì €ì¥ (ì„ë² ë”© í¬í•¨)

ì´ˆë³´ì ê°€ì´ë“œ:
- EMBEDDING_PROVIDERì— ë”°ë¼ ë²¡í„° í¬ê¸°ê°€ ë‹¬ë¼ì§
  - local-cpu (all-MiniLM-L6-v2): 384ì°¨ì›
  - local-gpu (bge-m3): 1024ì°¨ì›
  - openai (text-embedding-3-small): 1536ì°¨ì›
- save_incident(): ë¶„ì„ ê²°ê³¼ë¥¼ ì„ë² ë”©í•˜ì—¬ Qdrantì— ì €ì¥
"""

from qdrant_client import QdrantClient
from qdrant_client.http import models
from app.core.config import settings
from app.services.embedding_client import embedding_client
from app.services.clickhouse_client import ch_client
from datetime import datetime, timedelta
from typing import List, Optional
import uuid


def _get_vector_size() -> int:
    """
    í˜„ì¬ ì„ë² ë”© í”„ë¡œë°”ì´ë”ì— ë§ëŠ” ë²¡í„° í¬ê¸° ë°˜í™˜

    Returns:
        int: ë²¡í„° ì°¨ì› ìˆ˜
    """
    provider = settings.EMBEDDING_PROVIDER
    if provider == "local-cpu":
        # sentence-transformers/all-MiniLM-L6-v2 = 384ì°¨ì›
        return 384
    elif provider == "local-gpu":
        # BAAI/bge-m3 = 1024ì°¨ì›
        return 1024
    elif provider == "openai":
        # text-embedding-3-small = 1536ì°¨ì›
        return 1536
    else:
        # ê¸°ë³¸ê°’ (all-MiniLM-L6-v2)
        return 384


class RageEngine:
    """
    RAG ì—”ì§„ í´ë˜ìŠ¤ - ë²¡í„° ê²€ìƒ‰ ë° ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
    """

    def __init__(self):
        self.qdrant = QdrantClient(host=settings.QDRANT_HOST, port=settings.QDRANT_PORT)
        self.collection_name = "incident_manuals"
        self.normal_patterns_collection = "normal_log_patterns"
        self.anomaly_patterns_collection = "anomaly_log_patterns"
        self.incident_resolutions_collection = "incident_resolutions"
        self.vector_size = _get_vector_size()
        self._init_qdrant()

    def _init_qdrant(self):
        """
        Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
        - incident_manuals: ê¸°ì¡´ ì‚¬ë¡€/ë§¤ë‰´ì–¼ (í•˜ìœ„ í˜¸í™˜ì„±)
        - normal_log_patterns: ì •ìƒ ë¡œê·¸ íŒ¨í„´
        - anomaly_log_patterns: ë¹„ì •ìƒ ë¡œê·¸ íŒ¨í„´
        - incident_resolutions: í•´ê²°ëœ ì¸ì‹œë˜íŠ¸ ì‚¬ë¡€ (ê³¼ê±° í•´ê²° ë°©ë²•)
        """
        # 1. incident_manuals ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        try:
            existing = self.qdrant.get_collection(self.collection_name)
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ë²¡í„° í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ì¬ìƒì„±
            existing_size = existing.config.params.vectors.size
            if existing_size != self.vector_size:
                print(f"âš ï¸ ë²¡í„° í¬ê¸° ë¶ˆì¼ì¹˜ ê°ì§€: ê¸°ì¡´={existing_size}, í•„ìš”={self.vector_size}")
                print(f"ğŸ”„ ì»¬ë ‰ì…˜ '{self.collection_name}' ì¬ìƒì„± ì¤‘...")
                self.qdrant.delete_collection(self.collection_name)
                raise Exception("Recreate collection")
        except Exception:
            # ì»¬ë ‰ì…˜ ìƒì„± (ì„ë² ë”© í”„ë¡œë°”ì´ë”ì— ë§ëŠ” ë²¡í„° í¬ê¸° ì‚¬ìš©)
            self.qdrant.create_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=self.vector_size,
                    distance=models.Distance.COSINE
                ),
            )
            print(f"âœ… Qdrant ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {self.collection_name} (ë²¡í„° í¬ê¸°: {self.vector_size})")

        # 2. normal_log_patterns ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self._init_or_recreate_collection(self.normal_patterns_collection)

        # 3. anomaly_log_patterns ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self._init_or_recreate_collection(self.anomaly_patterns_collection)

        # 4. incident_resolutions ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self._init_or_recreate_collection(self.incident_resolutions_collection)

    def _init_or_recreate_collection(self, collection_name: str):
        """
        ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ë˜ëŠ” ì¬ìƒì„± (ë²¡í„° í¬ê¸° ê²€ì¦)

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        """
        try:
            existing = self.qdrant.get_collection(collection_name)
            # ë²¡í„° í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ì¬ìƒì„±
            existing_size = existing.config.params.vectors.size
            if existing_size != self.vector_size:
                print(f"âš ï¸ {collection_name} ë²¡í„° í¬ê¸° ë¶ˆì¼ì¹˜: ê¸°ì¡´={existing_size}, í•„ìš”={self.vector_size}")
                print(f"ğŸ”„ ì»¬ë ‰ì…˜ '{collection_name}' ì¬ìƒì„± ì¤‘...")
                self.qdrant.delete_collection(collection_name)
                raise Exception("Recreate collection")
        except Exception:
            # ì»¬ë ‰ì…˜ ìƒì„±
            self.qdrant.create_collection(
                collection_name=collection_name,
                vectors_config=models.VectorParams(
                    size=self.vector_size,
                    distance=models.Distance.COSINE
                ),
            )
            print(f"âœ… Qdrant ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {collection_name} (ë²¡í„° í¬ê¸°: {self.vector_size})")

    # ==================== Pattern Search Methods ====================

    async def search_patterns(self, collection_name: str, query_vector: List[float], limit: int = 3):
        """
        íŒ¨í„´ ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬ íŒ¨í„´ ê²€ìƒ‰

        Args:
            collection_name: 'normal_log_patterns' ë˜ëŠ” 'anomaly_log_patterns'
            query_vector: ì¿¼ë¦¬ ë²¡í„°
            limit: ìƒìœ„ ê²°ê³¼ ê°œìˆ˜

        Returns:
            [(score, payload), ...] ë¦¬ìŠ¤íŠ¸
        """
        try:
            results = self.qdrant.search(
                collection_name=collection_name,
                query_vector=query_vector,
                limit=limit
            )
            return [{"score": hit.score, "payload": hit.payload, "id": hit.id} for hit in results]
        except Exception as e:
            print(f"âŒ íŒ¨í„´ ê²€ìƒ‰ ì‹¤íŒ¨ ({collection_name}): {e}")
            return []

    async def save_pattern_batch(self, collection_name: str, patterns: List[dict]) -> List[str]:
        """
        ì—¬ëŸ¬ íŒ¨í„´ì„ ë°°ì¹˜ë¡œ ì €ì¥

        Args:
            collection_name: 'normal_log_patterns' ë˜ëŠ” 'anomaly_log_patterns'
            patterns: [
                {
                    "template_id": int,
                    "log_template": str,
                    "representative_message": str,
                    "log_level": str,
                    "service": str,
                    "keywords": List[str]
                },
                ...
            ]

        Returns:
            ì €ì¥ëœ íŒ¨í„´ ID ë¦¬ìŠ¤íŠ¸
        """
        if not patterns:
            return []

        # 1. ëª¨ë“  íŒ¨í„´ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì„ë² ë”©ìš©)
        texts = [f"{p['log_template']}\n\n{p['representative_message']}" for p in patterns]

        # 2. ë°°ì¹˜ ì„ë² ë”©
        vectors = await embedding_client.embed_documents(texts)

        # 3. Qdrant Point êµ¬ì„±
        points = []
        pattern_ids = []
        for pattern, vector in zip(patterns, vectors):
            point_id = str(uuid.uuid4())
            pattern_ids.append(point_id)

            payload = {
                "template_id": pattern["template_id"],
                "log_template": pattern["log_template"],
                "representative_message": pattern["representative_message"],
                "log_level": pattern["log_level"],
                "service": pattern["service"],
                "keywords": pattern.get("keywords", []),
                "label_source": pattern.get("label_source", "auto"),
                "sample_count": pattern.get("sample_count", 1),
                "first_seen": pattern.get("first_seen", datetime.now().isoformat()),
                "last_seen": pattern.get("last_seen", datetime.now().isoformat())
            }

            # anomaly_log_patterns ì»¬ë ‰ì…˜ì¼ ê²½ìš° ì¶”ê°€ í•„ë“œ
            if "anomaly_type" in pattern:
                payload["anomaly_type"] = pattern["anomaly_type"]
            if "severity" in pattern:
                payload["severity"] = pattern["severity"]

            points.append(
                models.PointStruct(
                    id=point_id,
                    vector=vector,
                    payload=payload
                )
            )

        # 4. Qdrantì— ë°°ì¹˜ ì €ì¥
        self.qdrant.upsert(
            collection_name=collection_name,
            points=points
        )

        print(f"âœ… íŒ¨í„´ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {collection_name} ({len(patterns)}ê±´)")
        return pattern_ids

    def delete_pattern(self, collection_name: str, point_id: str):
        """
        íŒ¨í„´ ì‚­ì œ

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            point_id: ì‚­ì œí•  Point ID
        """
        try:
            self.qdrant.delete(
                collection_name=collection_name,
                points_selector=models.PointIdsList(ids=[point_id])
            )
            print(f"âœ… íŒ¨í„´ ì‚­ì œ ì™„ë£Œ: {collection_name}/{point_id}")
        except Exception as e:
            print(f"âŒ íŒ¨í„´ ì‚­ì œ ì‹¤íŒ¨: {e}")

    async def search_similar_incidents(self, query_log: str, limit: int = 3):
        """Search for similar past incidents in Qdrant."""
        query_vector = await embedding_client.embed_query(query_log)
        
        results = self.qdrant.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=limit
        )
        return [{"score": hit.score, "payload": hit.payload} for hit in results]

    def get_log_context(self, timestamp, window_minutes: int = 5):
        """
        Fetch logs from ClickHouse around the anomaly timestamp.

        Args:
            timestamp: datetime ê°ì²´ ë˜ëŠ” ISO format ë¬¸ìì—´
            window_minutes: ì¡°íšŒí•  ì‹œê°„ ìœˆë„ìš° (ë¶„)

        Returns:
            í¬ë§·ëœ ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        # timestampê°€ ë¬¸ìì—´ì´ë©´ datetimeìœ¼ë¡œ ë³€í™˜
        if isinstance(timestamp, str):
            try:
                timestamp = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            except ValueError:
                timestamp = datetime.now()

        start_time = timestamp - timedelta(minutes=window_minutes)
        end_time = timestamp + timedelta(minutes=window_minutes)

        # ClickHouse í˜¸í™˜ datetime í¬ë§· (YYYY-MM-DD HH:MM:SS)
        start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')
        end_str = end_time.strftime('%Y-%m-%d %H:%M:%S')

        query = f"""
        SELECT timestamp, log_level, service, raw_message
        FROM logs
        WHERE timestamp BETWEEN '{start_str}' AND '{end_str}'
        ORDER BY timestamp
        LIMIT 100
        """
        result = ch_client.client.execute(query)
        # Format as string
        context_str = "\n".join([f"[{row[0]}] {row[1]} {row[2]}: {row[3]}" for row in result])
        return context_str

    async def save_incident(
        self,
        title: str,
        content: str,
        incident_type: str = "analysis",
        keywords: Optional[List[str]] = None,
        source: str = "chat",
        metadata: Optional[dict] = None
    ) -> str:
        """
        ì¥ì•  ì‚¬ë¡€/ë¶„ì„ ê²°ê³¼ë¥¼ Qdrantì— ì €ì¥

        Args:
            title: ì‚¬ë¡€ ì œëª© (ì˜ˆ: "NPM/AM-06 ì¸ì‹ ì˜¤ë¥˜ ë¶„ì„")
            content: ë¶„ì„ ë‚´ìš© (LLM ì‘ë‹µ ë“±)
            incident_type: ì‚¬ë¡€ ìœ í˜• (analysis, anomaly, manual)
            keywords: ê´€ë ¨ í‚¤ì›Œë“œ ëª©ë¡
            source: ì €ì¥ ì†ŒìŠ¤ (chat, agent, manual)
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
            ì €ì¥ëœ ë¬¸ì„œì˜ ID (UUID)
        """
        # 1. ì„ë² ë”© ìƒì„± (ì œëª© + ë‚´ìš© ê²°í•©)
        text_to_embed = f"{title}\n\n{content}"
        vector = await embedding_client.embed_query(text_to_embed)

        # 2. ë¬¸ì„œ ID ìƒì„±
        doc_id = str(uuid.uuid4())

        # 3. í˜ì´ë¡œë“œ êµ¬ì„±
        payload = {
            "title": title,
            "content": content,
            "type": incident_type,
            "keywords": keywords or [],
            "source": source,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }

        # 4. Qdrantì— ì €ì¥
        self.qdrant.upsert(
            collection_name=self.collection_name,
            points=[
                models.PointStruct(
                    id=doc_id,
                    vector=vector,
                    payload=payload
                )
            ]
        )

        print(f"âœ… Qdrant ì €ì¥ ì™„ë£Œ: {title} (ID: {doc_id})")
        return doc_id

    async def save_incidents_batch(
        self,
        documents: List[dict]
    ) -> List[str]:
        """
        ì—¬ëŸ¬ ì¥ì•  ì‚¬ë¡€ë¥¼ ë°°ì¹˜ë¡œ Qdrantì— ì €ì¥

        Args:
            documents: ë¬¸ì„œ ëª©ë¡ [{"title": ..., "content": ..., ...}, ...]

        Returns:
            ì €ì¥ëœ ë¬¸ì„œ ID ëª©ë¡
        """
        if not documents:
            return []

        # 1. ëª¨ë“  ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [f"{doc['title']}\n\n{doc['content']}" for doc in documents]

        # 2. ë°°ì¹˜ ì„ë² ë”©
        vectors = await embedding_client.embed_documents(texts)

        # 3. í¬ì¸íŠ¸ êµ¬ì„±
        points = []
        doc_ids = []
        for doc, vector in zip(documents, vectors):
            doc_id = str(uuid.uuid4())
            doc_ids.append(doc_id)

            payload = {
                "title": doc.get("title", ""),
                "content": doc.get("content", ""),
                "type": doc.get("incident_type", "analysis"),
                "keywords": doc.get("keywords", []),
                "source": doc.get("source", "batch"),
                "timestamp": datetime.now().isoformat(),
                "metadata": doc.get("metadata", {})
            }

            points.append(
                models.PointStruct(
                    id=doc_id,
                    vector=vector,
                    payload=payload
                )
            )

        # 4. Qdrantì— ë°°ì¹˜ ì €ì¥
        self.qdrant.upsert(
            collection_name=self.collection_name,
            points=points
        )

        print(f"âœ… Qdrant ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {len(documents)}ê±´")
        return doc_ids

    def get_incident_count(self) -> int:
        """
        ì €ì¥ëœ ì‚¬ë¡€ ìˆ˜ ì¡°íšŒ

        Returns:
            ì €ì¥ëœ ë¬¸ì„œ ìˆ˜
        """
        try:
            collection_info = self.qdrant.get_collection(self.collection_name)
            return collection_info.points_count
        except Exception:
            return 0

    # ==================== Incident Resolution Methods ====================

    async def search_resolutions(self, query_text: str, limit: int = 5):
        """
        ê³¼ê±° í•´ê²° ì‚¬ë¡€ ê²€ìƒ‰

        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬ (ì´ìƒ íƒì§€ ìƒì„¸ ì •ë³´)
            limit: ìƒìœ„ ê²°ê³¼ ê°œìˆ˜

        Returns:
            ìœ ì‚¬ í•´ê²° ì‚¬ë¡€ ëª©ë¡ [{score, payload, id}, ...]
        """
        try:
            query_vector = await embedding_client.embed_query(query_text)

            results = self.qdrant.search(
                collection_name=self.incident_resolutions_collection,
                query_vector=query_vector,
                limit=limit
            )

            return [
                {
                    "score": hit.score,
                    "payload": hit.payload,
                    "id": hit.id
                }
                for hit in results
            ]
        except Exception as e:
            print(f"âŒ í•´ê²° ì‚¬ë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    async def save_resolution(
        self,
        incident_summary: str,
        resolution_text: str,
        resolved_by: str,
        anomaly_score: float,
        service: str = "",
        template_id: Optional[int] = None,
        severity: str = "warning",
        metadata: Optional[dict] = None
    ) -> str:
        """
        í•´ê²° ì •ë³´ë¥¼ Qdrant incident_resolutions ì»¬ë ‰ì…˜ì— ì €ì¥

        Args:
            incident_summary: ì¸ì‹œë˜íŠ¸ ìš”ì•½ (ìƒì„¸ ì •ë³´)
            resolution_text: í•´ê²° ë°©ë²• ìƒì„¸ ì„¤ëª…
            resolved_by: í•´ê²°ì ì´ë¦„
            anomaly_score: ì´ìƒ ì ìˆ˜ (0.0 ~ 1.0)
            service: ì„œë¹„ìŠ¤ëª…
            template_id: Drain3 í…œí”Œë¦¿ ID
            severity: ì‹¬ê°ë„ (critical, warning, info)
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
            ì €ì¥ëœ Point ID (UUID)
        """
        try:
            # 1. ì„ë² ë”© ìƒì„± (ì¸ì‹œë˜íŠ¸ ìš”ì•½ + í•´ê²° ë‚´ìš©)
            text_to_embed = f"{incident_summary}\n\ní•´ê²° ë°©ë²•: {resolution_text}"
            vector = await embedding_client.embed_query(text_to_embed)

            # 2. Point ID ìƒì„±
            point_id = str(uuid.uuid4())

            # 3. Payload êµ¬ì„±
            payload = {
                "incident_summary": incident_summary,
                "resolution": resolution_text,
                "resolved_by": resolved_by,
                "anomaly_score": anomaly_score,
                "service": service,
                "template_id": template_id,
                "severity": severity,
                "timestamp": datetime.now().isoformat(),
                "metadata": metadata or {}
            }

            # 4. Qdrantì— ì €ì¥
            self.qdrant.upsert(
                collection_name=self.incident_resolutions_collection,
                points=[
                    models.PointStruct(
                        id=point_id,
                        vector=vector,
                        payload=payload
                    )
                ]
            )

            print(f"âœ… í•´ê²° ì‚¬ë¡€ ì €ì¥ ì™„ë£Œ: {incident_summary[:50]}... (ID: {point_id})")
            return point_id

        except Exception as e:
            print(f"âŒ í•´ê²° ì‚¬ë¡€ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""


rag_engine = RageEngine()
